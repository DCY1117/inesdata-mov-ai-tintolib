# INESData Dataspace: AI-Powered Data Services with Policy-Compliant Data Exchange

[![INESData](https://img.shields.io/badge/INESData-Dataspace-blue)](https://github.com/INESData)
[![Python](https://img.shields.io/badge/Python-3.10+-green.svg)](https://www.python.org/downloads/)
[![Streamlit](https://img.shields.io/badge/Streamlit-App-red.svg)](https://streamlit.io/)
[![TINTOlib](https://img.shields.io/badge/TINTOlib-ML-orange.svg)](https://github.com/oeg-upm/TINTOlib)
[![EDC](https://img.shields.io/badge/EDC-Connector-purple.svg)](https://github.com/eclipse-edc/connector)

A comprehensive demonstration of building **data services integrated with an INESData dataspace** using the **EMT (EstaciÃ³n de Monitoreo de TrÃ¡fico) dataset** as a real-world example. This project showcases how to create intelligent services that automatically consume policy-compliant data from a distributed dataspace.

## ğŸ¯ Overview

This repository demonstrates a **complete dataspace-driven service architecture** with:
- **Data Service Pattern**: A Streamlit application acting as a dataspace-connected service that consumes data through connectors
- **Policy-Compliant Data Access**: Automatic authentication, contract negotiation, and data transfer respecting dataspace policies
- **Real-World EMT Dataset**: Using EMT traffic monitoring data generated via [inesdata-mov-data-generation](https://github.com/INESData/inesdata-mov-data-generation)
- **AI/ML Integration**: TINTOlib transforms tabular EMT data into synthetic images for computer vision models
- **Connector-Based Integration**: Provider and Consumer EDC connectors managing secure data exchange
- **Automated Workflows**: End-to-end pipelines from data discovery â†’ policy evaluation â†’ transfer â†’ processing

## ğŸ—ï¸ Architecture: Data Service Connected to Dataspace

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INESData Dataspace                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Provider    â”‚   Central Authority    â”‚   Consumer Connector        â”‚
â”‚  Connector   â”‚   (Keycloak, Catalog)  â”‚   (Our Service Access)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           Policy Framework (Contract Negotiation)            â”‚  â”‚
â”‚  â”‚  â€¢ Usage policies â€¢ Data sovereignty â€¢ Access control        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â”‚ (Policy-compliant data exchange)
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Streamlit Data Service â”‚
                    â”‚  (Consumer Application) â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚               â”‚                 â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  EMT Data from  â”‚ â”‚ Contract â”‚ â”‚  TINTOlib AI    â”‚
        â”‚  Dataspace      â”‚ â”‚ Manager  â”‚ â”‚  Synthesizer    â”‚
        â”‚  (via Connector)â”‚ â”‚          â”‚ â”‚  (Vision Models)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Innovation**: This example demonstrates how to build **intelligent services that are consumers within a dataspace**, automatically discovering, negotiating, and accessing policy-compliant data without manual intervention.

## ğŸ“¦ Components

This project integrates the following INESData components:

| Component | Repository | Description |
|-----------|-----------|-------------|
| **Connector** | [inesdata-connector](https://github.com/INESData/inesdata-connector) | Core EDC-based connector for data exchange |
| **Deployment** | [inesdata-deployment](https://github.com/INESData/inesdata-deployment) | Infrastructure deployment scripts and configurations |
| **Public Portal Frontend** | [inesdata-public-portal-frontend](https://github.com/INESData/inesdata-public-portal-frontend) | Web interface for dataspace exploration |
| **Public Portal Backend** | [inesdata-public-portal-backend](https://github.com/INESData/inesdata-public-portal-backend) | Backend services for portal functionality |
| **Registration Service** | [inesdata-registration-service](https://github.com/INESData/inesdata-registration-service) | Connector registration and management |

## ğŸ’¡ Use Case: EMT Traffic Data â†’ AI Vision Models

This project uses **EMT (EstaciÃ³n de Monitoreo de TrÃ¡fico) traffic monitoring data** as a real-world example of how organizations can:

1. **Share data through dataspaces** while maintaining control via policies
2. **Build intelligent services** that automatically consume policy-compliant data
3. **Add value** through AI/ML transformations (synthetic image generation)
4. **Monetize or collaborate** using dataspace frameworks

### The EMT Dataset

### Step 1: Data Generation (EMT Dataset)
The workflow starts with **EMT traffic monitoring data** generated using [inesdata-mov-data-generation](https://github.com/INESData/inesdata-mov-data-generation). This dataset is exposed through the **Provider Connector** within the dataspace.

**Key Point**: Any data provider can expose their datasets following dataspace standards and policies.

### Step 2: Authentication & Authorization
Secure login via Keycloak using consumer connector credentials. The app manages OAuth2 tokens for all dataspace interactions. This ensures only authorized services can access data.

![Authentication Screen](docs/images/1_authentication.png)

### Step 3: Federated Catalog Discovery
Browse the **federated catalog** to discover available datasets exposed by any Provider Connector. The service automatically discovers the EMT dataset and other available resources.

![Catalog Browser](docs/images/2_catalog_browser.png)

**Pattern**: This demonstrates how services can **automatically discover and catalog available data** without manual configuration.

### Step 4: Policy-Aware Contract Negotiation
View detailed metadata and **automatically negotiate usage contracts**. The system evaluates data access policies:
- Who can access the data?
- How can the data be used?
- What processing is allowed?
- Are there time/usage restrictions?

The contract is negotiated programmatically before access is granted.

![Dataset Details](docs/images/3_dataset_details.png)

**Innovation**: Policies are evaluated automatically, enabling services to **respect data sovereignty** without manual approval workflows.

### Step 5: Secure Policy-Compliant Data Transfer
Once the contract is agreed upon, the **EMT dataset (CSV) is securely transferred via EDC protocols**. The system ensures:
- Only authorized services receive the data
- Transfer is encrypted and traceable
- Access is logged and auditable
- Policies continue to apply to the received data

![Downloaded Datasets](docs/images/4_data_transfer.png)

### Step 6: AI-Powered Data Processing with TINTOlib
The tabular EMT data is processed using **TINTOlib** to generate synthetic images. This demonstrates **adding value through AI while respecting data policies**:

- **Input**: Tabular EMT traffic metrics (vehicle counts, speeds, congestion patterns, etc.)
- **Process**: TINTOlib converts features to synthetic images representing the feature space
- **Output**: High-dimensional images ready for CNNs, Vision Transformers, or other deep learning models

![Synthetic Image Generation](docs/images/5_synthetic_generation.png)

### Step 7: Generated Vision Dataset Ready for ML
The synthetic images represent the complete feature space of the EMT data, enabling training of computer vision models.

![Synthetic Images Output](docs/images/6_synthetic_images.png)

**Business Value**: Organizations can:
- Train vision models on private data without sharing raw records
- Create new data products (synthetic images) from shared data
- Maintain compliance with data policies throughout the process
- Build AI services that rely on dataspace-sourced data

---

---

## ğŸ”§ Building Data Services Connected to Dataspaces: The Pattern

This project exemplifies a **reusable service architecture pattern** for dataspace-integrated applications. Here's how to build similar services:

### Service Integration Pattern

```python
# Your Data Service Architecture
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Your Intelligent Data Service                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  1. Service Bootstrap & Config                               â”‚
â”‚     â””â”€ Initialize consumer connector credentials             â”‚
â”‚     â””â”€ Load dataspace catalog endpoint                       â”‚
â”‚                                                               â”‚
â”‚  2. Data Discovery Module                                    â”‚
â”‚     â””â”€ Query federated catalog (async)                       â”‚
â”‚     â””â”€ Filter datasets by metadata/domain                    â”‚
â”‚     â””â”€ Parse available contracts & policies                  â”‚
â”‚                                                               â”‚
â”‚  3. Policy Evaluation Engine                                 â”‚
â”‚     â””â”€ Check data access policies automatically              â”‚
â”‚     â””â”€ Verify service compliance requirements                â”‚
â”‚     â””â”€ Negotiate contracts programmatically                  â”‚
â”‚                                                               â”‚
â”‚  4. Secure Data Ingestion                                    â”‚
â”‚     â””â”€ Request data via EDC consumer connector               â”‚
â”‚     â””â”€ Manage transfer tokens & encryption                   â”‚
â”‚     â””â”€ Validate received data integrity                      â”‚
â”‚                                                               â”‚
â”‚  5. Business Logic & AI Processing                           â”‚
â”‚     â””â”€ Apply your domain logic (TINTOlib in this example)    â”‚
â”‚     â””â”€ Add value while respecting data policies              â”‚
â”‚     â””â”€ Generate insights/products from dataspace data        â”‚
â”‚                                                               â”‚
â”‚  6. Result Management                                        â”‚
â”‚     â””â”€ Store processed outputs                               â”‚
â”‚     â””â”€ Optionally expose new datasets to dataspace           â”‚
â”‚     â””â”€ Maintain audit trail of data usage                    â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Capabilities Your Service Gains

| Capability | Benefit |
|-----------|---------|
| **Automatic Data Discovery** | Services find new data sources without reconfiguration |
| **Policy Compliance** | Access control is automatic, not manual |
| **Secure Exchange** | EDC handles encryption, authentication, and audit logs |
| **Scalability** | Add more data providers/consumers transparently |
| **Interoperability** | Works with any dataspace using EDC standards |
| **Trust & Governance** | Data sovereignty maintained throughout pipeline |

### Real-World Example: This Project

This repository implements exactly this pattern:
- **Service**: Streamlit application acts as consumer service
- **Discovery**: Queries catalog for EMT datasets
- **Data Transfer**: Securely receives CSV from provider
- **Processing**: Applies TINTOlib transformation
- **Output**: Generates and manages synthetic images

---

## ğŸš€ Quick Start

### Prerequisites

- **Kubernetes cluster** (Minikube recommended for local deployment)
- **Docker** with Minikube tunnel running
- **kubectl** and **helm** installed
- **Python 3.10+**

### ğŸ“˜ Deployment Guides

This repository includes comprehensive documentation for local deployment:

- **ğŸ“„ [Instalar-Inesdata-DEV-localmente-v2.pdf](./Instalar-Inesdata-DEV-localmente-v2.pdf)** - Complete step-by-step guide for local INESData deployment (Spanish)
- **ğŸ“ [deployment-guide.txt](./inesdata-deployment/deployment-guide.txt)** - Quick reference with deployment commands
- **ğŸ“‹ [Guia_Despliegue_Local_INESData.docx](./Guia_Despliegue_Local_INESData.docx)** - Local deployment documentation (Spanish)
- **ğŸ“‹ [INESData_MOV_Guide.docx](./INESData_MOV_Guide.docx)** - INESData Mobile data generation guide

### Deployment Steps

For detailed step-by-step instructions, refer to the comprehensive deployment guides:

- **[Instalar-Inesdata-DEV-localmente-v2.pdf](./Instalar-Inesdata-DEV-localmente-v2.pdf)** - Complete local deployment walkthrough with all prerequisites and configurations
- **[Guia_Despliegue_Local_INESData.docx](./Guia_Despliegue_Local_INESData.docx)** - Additional deployment notes and troubleshooting

#### Quick Reference Commands

1. **Deploy common services** (PostgreSQL, MinIO, Keycloak, Vault)
   ```bash
   cd inesdata-deployment/common
   helm install -f values.yaml -n common --create-namespace common-services .
   ```

2. **Create dataspace and connectors**
   ```bash
   cd inesdata-deployment
   source .venv/bin/activate
   python deployer.py dataspace create <dataspace-name>
   python deployer.py connector create <connector-provider> <dataspace-name>
   python deployer.py connector create <connector-consumer> <dataspace-name>
   ```

3. **Deploy using Helm charts** (follow detailed steps in PDF guide)
   ```bash
   # Deploy registration service (step 1)
   cd dataspace/step-1
   helm install -f values.yaml -n <dataspace-name>-ds --create-namespace <dataspace-name>-dataspace-s1 .
   
   # Deploy connectors
   cd ../../connector
   helm install -f values.yaml -n <dataspace-name>-ds <connector-name> .
   ```

4. **Launch Streamlit application**
   ```bash
   cd streamlit-dataspace-app
   pip install -r requirements.txt
   streamlit run app.py
   ```

### ğŸ³ Using Local Docker Images

Instead of pulling images from GitHub Container Registry (`ghcr.io/inesdata/*`), you can build and use local images:

1. **Build images locally** for each component:
   ```bash
   # Build connector image
   cd inesdata-connector
   docker build -f docker/Dockerfile -t inesdata-connector:local .
   
   # Build connector interface
   cd ../inesdata-connector-interface
   docker build -f docker/Dockerfile -t inesdata-connector-interface:local .
   
   # Build registration service
   cd ../inesdata-registration-service
   docker build -f docker/Dockerfile -t inesdata-registration-service:local .
   
   # Build public portal frontend
   cd ../inesdata-public-portal-frontend
   docker build -f docker/Dockerfile -t inesdata-public-portal-frontend:local .
   ```

2. **Update Helm values** to use local images:
   - In `inesdata-deployment/connector/values.yaml.tpl`, change:
     ```yaml
     image:
       name: inesdata-connector:local
       pullPolicy: Never
     ```
   - Apply similar changes to other component values files
   
   > See deployment guides for complete instructions on local image configuration

## ğŸ’¡ Use Case: EMT Traffic Data â†’ AI Vision Models

The Streamlit application demonstrates an AI workflow using real EMT traffic data:

1. **Authenticate** with Keycloak using consumer connector credentials
2. **Browse** the federated catalog to discover available datasets
3. **Negotiate** contracts automatically with the provider
4. **Transfer** and download tabular datasets (CSV) with policy compliance
5. **Transform** data using TINTOlib into synthetic images
6. **Generate** vision-ready datasets for CNN/transformer models

### TINTOlib Integration

[TINTOlib](https://github.com/oeg-upm/TINTOlib) converts tabular data into images using various methods:
- **TINTO**: Feature-to-pixel mapping
- **IGTD**: Image Generator from Tabular Data
- **BarGraph**: Visual bar representations
- **DistanceMatrix**: Similarity-based visualizations
- And more...

This enables the use of powerful computer vision models (ResNet, Vision Transformers, etc.) on traditionally tabular datasets.

## ğŸ“š How to Build Your Own Dataspace-Connected Service

This project serves as a **template** for building intelligent services that integrate with dataspaces. Here's how to adapt it for your domain:

### Step 1: Set Up Dataspace Access
1. **Configure connector credentials** in your service configuration
2. **Implement authentication** using Keycloak or your identity provider
3. **Initialize the EDC Management API client** to interact with the consumer connector

### Step 2: Implement Data Discovery
```python
async def discover_relevant_data(domain: str) -> List[Dataset]:
    """Query catalog for datasets in your domain"""
    catalog = await connector.query_catalog(
        filters={"domain": domain}
    )
    return [d for d in catalog if meets_your_criteria(d)]
```

**Options**:
- Filter by dataset metadata (domain, format, frequency)
- Use semantic search if catalog supports it
- Subscribe to catalog updates for new datasets

### Step 3: Add Policy Evaluation
```python
async def can_use_data(dataset_id: str) -> bool:
    """Check if dataset policies allow your use case"""
    policies = await connector.get_policies(dataset_id)
    
    # Evaluate against your service requirements
    # Examples: can we train ML models? Can we redistribute?
    return your_business_logic_accepts(policies)
```

**Policy Examples**:
- Usage restrictions (training only, analytics only, etc.)
- Geographic restrictions (GDPR compliance, etc.)
- Temporal restrictions (valid until date, etc.)
- Commercial terms (free, licensed, etc.)

### Step 4: Integrate Your Business Logic
Replace or augment the TINTOlib processing with your own:
```python
async def process_policy_compliant_data(data: pd.DataFrame) -> Result:
    """Apply your AI/ML/business logic to dataspace data"""
    # Examples:
    # - Machine learning model inference
    # - Data aggregation and analytics
    # - Image/signal processing
    # - Anomaly detection
    # - Data enrichment
    return await your_processing_pipeline(data)
```

### Step 5: Manage Results
- Store processed results securely
- Optionally expose new datasets back to the dataspace
- Maintain audit trails of data usage and transformations
- Log policy compliance for compliance teams

### Real-World Service Examples

| Service Type | Data Needed | Processing | Output |
|--------------|-----------|-----------|--------|
| **Traffic Analytics** | EMT data (this project) | TINTOlib â†’ Image synthesis | Vision ML models |
| **Predictive Maintenance** | Sensor data | ML model â†’ Predictions | Maintenance alerts |
| **Risk Assessment** | Financial data + Risk policies | Compliance checking | Risk scores |
| **Healthcare Insights** | Medical records (FHIR) | Privacy-preserving ML | Patient insights |
| **Smart City Dashboard** | Multiple IoT sources | Data fusion | Real-time visualizations |

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ Instalar-Inesdata-DEV-localmente-v2.pdf  # ğŸ“„ Main deployment guide
â”œâ”€â”€ Guia_Despliegue_Local_INESData.docx      # ğŸ“‹ Additional documentation
â”œâ”€â”€ inesdata-connector/                       # Connector source code
â”œâ”€â”€ inesdata-connector-interface/             # Connector web interface
â”œâ”€â”€ inesdata-deployment/                      # ğŸ”§ Deployment scripts and configs
â”‚   â”œâ”€â”€ deployment-guide.txt                  # Quick reference commands
â”‚   â”œâ”€â”€ deployer.py                           # Automated deployment tool
â”‚   â”œâ”€â”€ common/                               # Common services (DB, MinIO, etc.)
â”‚   â”œâ”€â”€ connector/                            # Connector Helm charts
â”‚   â””â”€â”€ dataspace/                            # Dataspace Helm charts
â”œâ”€â”€ inesdata-public-portal-frontend/          # Public portal UI
â”œâ”€â”€ inesdata-registration-service/            # Registration service
â””â”€â”€ streamlit-dataspace-app/                  # ğŸŒŸ AI Application (main showcase)
    â”œâ”€â”€ app.py                                # Main Streamlit interface
    â”œâ”€â”€ auth.py                               # Keycloak authentication
    â”œâ”€â”€ edc_client.py                         # EDC Management API client
    â”œâ”€â”€ tinto_processor.py                    # TINTOlib integration
    â””â”€â”€ config.py                             # Configuration
```

## ğŸ¨ Features

### Streamlit Application
- **ğŸ” Secure Authentication**: OAuth2/Keycloak integration
- **ğŸ“Š Catalog Browser**: Explore federated datasets with rich metadata
- **ğŸ¤ Automated Negotiation**: Contract handling with policy evaluation
- **ğŸ“¥ Data Transfer**: EDR-based secure data download
- **ğŸ¨ Image Synthesis**: Real-time TINTOlib transformations
- **ğŸ“ˆ Progress Tracking**: Visual feedback for async operations

### Dataspace Capabilities
- **Federated catalog** across multiple connectors
- **Policy-based access control** with contract negotiation
- **Secure data transfer** using EDC protocols
- **Keycloak authentication** for user and service accounts

## ğŸ› ï¸ Configuration

Key configuration in `streamlit-dataspace-app/config.py`:

```python
KEYCLOAK_URL = "http://keycloak.dev.ed.inesdata.upm"
CONSUMER_CONNECTOR_URL = "http://conn-oeg-consumer.dev.ds.inesdata.upm"
PROVIDER_CONNECTOR_URL = "http://conn-oeg-provider.dev.ds.inesdata.upm"
```

Credentials are provided via interactive login (no hardcoded secrets).

## ğŸ“š Documentation

### Deployment Documentation
- **ğŸ“„ [Instalar-Inesdata-DEV-localmente-v2.pdf](./Instalar-Inesdata-DEV-localmente-v2.pdf)** - Complete local deployment guide
- **ğŸ“ [deployment-guide.txt](./inesdata-deployment/deployment-guide.txt)** - Command reference for dataspace/connector deployment
- **ğŸ“– [inesdata-deployment/README.md](./inesdata-deployment/README.md)** - Deployment architecture overview
- **ğŸ“‹ [Guia_Despliegue_Local_INESData.docx](./Guia_Despliegue_Local_INESData.docx)** - Additional deployment notes

### Application Documentation
- **ğŸ¨ [streamlit-dataspace-app/README.md](./streamlit-dataspace-app/README.md)** - Streamlit application setup and usage

### External Resources
- **ğŸŒ [INESData GitHub Organization](https://github.com/INESData)** - Official INESData repositories
- **ğŸ“š [TINTOlib Documentation](https://github.com/oeg-upm/TINTOlib)** - Image synthesis library

## ğŸ¤ Acknowledgments

This project builds upon the **INESData** initiative:
- **INESData Team** for the dataspace infrastructure components
- **OEG-UPM** for TINTOlib library
- **Eclipse Dataspace Components** for the underlying EDC framework

## ğŸ“„ License

This project follows the licensing of its component repositories. Please refer to individual component licenses for details.

## ğŸ”— Related Links

- [INESData GitHub Organization](https://github.com/INESData)
- [TINTOlib Documentation](https://github.com/oeg-upm/TINTOlib)
- [Eclipse Dataspace Components](https://github.com/eclipse-edc/Connector)

---

**Note**: This is a demonstration environment for local development and testing. For production deployments, refer to the official INESData deployment documentation.

# Disclaimer

Este trabajo ha recibido financiaciÃ³n del proyecto INESData (Infraestructura para la INvestigaciÃ³n de ESpacios de DAtos distribuidos en UPM), un proyecto financiado en el contexto de la convocatoria UNICO I+D CLOUD del Ministerio para la TransformaciÃ³n Digital y de la FunciÃ³n PÃºblica en el marco del PRTR financiado por UniÃ³n Europea (NextGenerationEU)
